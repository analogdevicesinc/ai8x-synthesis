---
# HWC (little data) configuration for SVHN
# Parallel Model

arch: ai85tinierssdqr
dataset: qrcode_160_120_kpts_ext

layers:

  # Layer 0: fire1, inp: 3x320x240
  - out_offset: 0x1000
    in_offset: 0x0000
    processors: 0x0000000000000007
    output_processors: 0xffffffff00000000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    streaming: true

  # Layer 1: fire2, inp: 32x320x240
  - out_offset: 0x1000
    in_offset: 0x1000
    processors: 0xffffffff00000000
    output_processors: 0x00000000ffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    streaming: true

  # Layer 2: fire3, inp: 32x320x240
  - out_offset: 0x2440 #0x2000
    in_offset: 0x1000
    processors: 0x00000000ffffffff
    output_processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    max_pool: 2
    pool_stride: 2
    streaming: true

  # Layer 3: fire3_1, inp: 64x160x120
  - out_offset: 0x3000
    in_offset: 0x2440
    processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    max_pool: 2
    pool_stride: 2
    streaming: true

  # Layer 4: fire4, inp: 64x80x60
  - out_offset: 0x4000 
    in_offset: 0x3000
    processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    streaming: true

  # Layer 5: fire5, inp: 64x80x60
  - out_offset: 0x5000 # 0x12C0
    in_offset: 0x4000
    processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    max_pool: 2
    pool_stride: 2
    streaming: true

  # Layer 6: fire6, inp: 64x40x30
  - out_offset: 0x6400  # 0x12C0
    in_offset: 0x5000
    processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 7: fire7, inp: 64x40x30
  - out_offset: 0x0000  # 0x2580
    in_offset: 0x6400
    processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 8: fire8, inp: 128x40x30
  - name: fire8
    out_offset: 0x3000  # 0x12C0
    in_offset: 0x0000
    processors: 0xffffffffffffffff
    output_processors: 0x00000000ffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 9: fire9, inp: 32x40x30
  - name: fire9
    out_offset: 0x3000  # 0x04B0
    in_offset: 0x3000
    processors: 0x00000000ffffffff
    output_processors: 0xffffffff00000000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    max_pool: 2
    pool_stride: 2

  # Layer 10: fire10, inp: 32x20x15
  - name: fire10
    out_offset: 0x4400  # 0x0280
    in_offset: 0x3000
    processors: 0xffffffff00000000
    output_processors: 0x00000000ffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    max_pool: 2
    pool_stride: 2

  # Layer 11: conv12_1, inp: 32x10x7
  - out_offset: 0x5000 # 0x0280
    in_offset: 0x4400
    processors: 0x00000000ffffffff
    output_processors: 0x000000000000ffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 12: conv12_2, inp: 16x10x7
  - name: conv12_2
    out_offset: 0x5000 # 0x003C
    in_offset: 0x5000
    processors: 0x000000000000ffff
    output_processors: 0x00000000ffff0000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    max_pool: 2
    pool_stride: 2

  # Layer 13: loc_conv8, inp: 32x40x30
  - out_offset: 0x5400  # 0x12C0
    in_offset: 0x3000
    processors: 0x00000000ffffffff
    output_processors: 0x000000000000ffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: fire8
    output: true
    activate: None

  # Layer 14: loc_conv9, inp: 32x20x15
  - out_offset: 0x66C0  # 0x04B0
    in_offset: 0x3000
    processors: 0xffffffff00000000
    output_processors: 0x000000000000ffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: fire9
    output: true
    activate: None

  # Layer 15: loc_conv10, inp: 32x10x7
  - out_offset: 0x6B70 # 0x0118
    in_offset: 0x4400
    processors: 0x00000000ffffffff
    output_processors: 0x000000000000ffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: fire10
    output: true
    activate: None

  # Layer 16: loc_conv12_2, inp: 16x5x3
  - out_offset: 0x6C88 # 0x003C
    in_offset: 0x5000
    processors: 0x00000000ffff0000
    output_processors: 0x000000000000ffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: conv12_2
    output: true
    activate: None
 
  # Layer 17: kpts_fire8, inp: 32x40x30
  - out_offset: 0x4000 # 0x12C0
    in_offset: 0x3000
    processors: 0x00000000ffffffff
    output_processors: 0xffffffff00000000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: fire8
    output: true
    activate: None
  
  # Layer 18: kpts_fire9, inp: 32x20x15
  - out_offset: 0x52C0 # 0x04B0
    in_offset: 0x3000
    processors: 0xffffffff00000000
    output_processors: 0xffffffff00000000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: fire9
    output: true
    activate: None
  
  # Layer 19: kpts_fire10, inp: 32x10x7
  - out_offset: 0x5770 # 0x0118
    in_offset: 0x4400
    processors: 0x00000000ffffffff
    output_processors: 0xffffffff00000000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: fire10
    output: true
    activate: None
  
  # Layer 20: kpts_fire12_2, inp: 16x5x3
  - out_offset: 0x5888
    in_offset: 0x5000
    processors: 0x00000000ffff0000
    output_processors: 0xffffffff00000000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: conv12_2
    output: true
    activate: None
  
  # Layer 21: cl_fire8, inp: 32x40x30
  - out_offset: 0x5400  # 0x12C0
    in_offset: 0x3000
    processors: 0x00000000ffffffff
    output_processors: 0x0000000000ff0000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: fire8
    output: true
    activate: None

  # Layer 22: cl_conv9, inp: 32x20x15
  - out_offset: 0x66C0  # 0x04B0
    in_offset: 0x3000
    processors: 0xffffffff00000000
    output_processors: 0x0000000000ff0000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: fire9
    output: true
    activate: None

  # Layer 23: cl_conv10, inp: 32x10x7
  - out_offset: 0x6B70 # 0x0118
    in_offset: 0x4400
    processors: 0x00000000ffffffff
    output_processors: 0x0000000000ff0000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: fire10
    output: true
    activate: None

  # Layer 24: cl_conv12_2, inp: 16x5x3
  - out_offset: 0x6C88 # 0x003C
    in_offset: 0x5000
    processors: 0x00000000ffff0000
    output_processors: 0x0000000000ff0000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    in_sequences: conv12_2
    output: true
    activate: None
