---
# HWC (little data) configuration for CIFAR-100
# Simple Model

arch: ai85ressimplenet
dataset: CIFAR100

layers:
  # Layer 0
  - out_offset: 0x2000
    processors: 0x7000000000000000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    data_format: HWC

  # Layer 1
  - out_offset: 0x0000
    processors: 0x0ffff00000000000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 2 - re-form data with gap
  - out_offset: 0x2000
    processors: 0x00000000000fffff
    output_processors: 0x00000000000fffff
    operation: passthrough
    write_gap: 1

  # Layer 3
  - in_offset: 0x0000
    in_sequences: 1
    out_offset: 0x2004
    processors: 0x00000000000fffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    write_gap: 1

  # Layer 4 - Residual-1
  - in_sequences: [2, 3]
    in_offset: 0x2000
    out_offset: 0x0000
    processors: 0x00000000000fffff
    eltwise: add
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 5
  - out_offset: 0x2000
    processors: 0xfffff00000000000
    output_processors: 0x000000fffff00000
    max_pool: 2
    pool_stride: 2
    pad: 1
    operation: conv2d
    kernel_size: 3x3
    activate: ReLU

  # Layer 6 - re-form data with gap
  - out_offset: 0x0000
    processors: 0x000000fffff00000
    output_processors: 0x000000fffff00000
    op: passthrough
    write_gap: 1

  # Layer 7 (input offset 0x0000)
  - in_offset: 0x2000
    in_sequences: 5
    out_offset: 0x0004
    processors: 0x000000fffff00000
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    write_gap: 1

  # Layer 8 - Residual-2 (input offset 0x2000)
  - in_sequences: [6, 7]
    in_offset: 0x0000
    out_offset: 0x2000
    processors: 0x000000fffff00000
    eltwise: add
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 9
  - out_offset: 0x0000
    processors: 0x00000fffffffffff
    max_pool: 2
    pool_stride: 2
    pad: 1
    operation: conv2d
    kernel_size: 3x3
    activate: ReLU

  # Layer 10 - re-form data with gap
  - out_offset: 0x2000
    processors: 0x0000ffffffffffff
    output_processors: 0x0000ffffffffffff
    op: passthrough
    write_gap: 1

  # Layer 11
  - in_offset: 0x0000
    in_sequences: 9
    out_offset: 0x2004
    processors: 0x0000ffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    write_gap: 1

  # Layer 12 - Residual-3
  - in_sequences: [10, 11]
    in_offset: 0x2000
    out_offset: 0x0000
    processors: 0x0000ffffffffffff
    eltwise: add
    max_pool: 2
    pool_stride: 2
    pad: 1
    pool_first: false
    operation: conv2d
    kernel_size: 3x3
    activate: ReLU

  # Layer 13
  - out_offset: 0x2000
    processors: 0x0000ffffffffffff
    max_pool: 2
    pool_stride: 2
    pad: 0
    operation: conv2d
    kernel_size: 1x1
    activate: ReLU

  # Layer 14
  - out_offset: 0x0000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU

  # Layer 15
  - out_offset: 0x2000
    processors: 0xffffffffffffffff
    max_pool: 2
    pool_stride: 2
    pad: 1
    operation: conv2d
    kernel_size: 3x3
    activate: ReLU

  # Layer 16
  - out_offset: 0x0000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    output_width: 32
    activate: None
